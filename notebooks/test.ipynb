{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c114760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\loren\\\\OneDrive\\\\Desktop\\\\Workbench\\\\ai-knowledge-assistant'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "529ace1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miao\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "os.chdir(\"c:/Users/loren/OneDrive/Desktop/Workbench/ai-knowledge-assistant\")\n",
    "\n",
    "load_dotenv()  # Carica il file .env\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "a = os.getenv(\"PIPO\")\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39025c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key: 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\n"
     ]
    }
   ],
   "source": [
    "print(\"API Key:\", openai_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17898633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from backend.app.services.llm_service import llm_predict\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8226d7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "hf_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "hf_client = InferenceClient(token=hf_token)\n",
    "\n",
    "# definisco il modello llm\n",
    "llm_model = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54fed6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_predict(context: str, question: str) -> str:\n",
    "    # boh qui fa cose \n",
    "    p = f\"dato il contesto {context}, gestiscimi questa richiesta: {question}\"\n",
    "\n",
    "    messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Sei un assistente che risponde in Italiano alle richiesta dell'utente.\"},\n",
    "    {\"role\": \"user\", \"content\": p}\n",
    "    ]\n",
    "\n",
    "    completion = hf_client.chat_completion(model=llm_model, messages=messages)\n",
    "    output_text = completion.choices[0].message.content\n",
    "\n",
    "    return output_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "679d00e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sembra che non ci sia un contesto specifico fornito. Se vuoi, posso suggerirti alcuni contesti di prova e poi faremo un riassunto sulla tua richiesta una volta che avrai fornito una risposta.\\n\\nPer favore, scegli uno dei seguenti contesti:\\n\\n A) Un riassunto di un articolo di giornale\\n B) Un riassunto di un libro di storia\\n C) Un riassunto di un progetto di lavoro\\n D) Un riassunto di un evento sportivo\\n\\nScegli il contesto che preferisci e potrai fornire le informazioni necessarie per fare un riassunto.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_predict(\"questo è un contesto di prova\", \"fammi un riassunto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c062719f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test LLM Predictor ===\n",
      "\n",
      "✅ Token API trovato\n",
      "--- Test Funzione Standalone ---\n",
      "Contesto: \n",
      "    Python è un linguaggio di programmazione ad alto livello, interpretato e orientato agli oggetti...\n",
      "Domanda: Quali sono i principali utilizzi di Python?\n",
      "Risposta: I principali utilizzi di Python sono:\n",
      "\n",
      "- Data Science: Python è ampiamente utilizzato nella data science per la raccolta, analisi e visualizzazione dei dati.\n",
      "- Machine Learning: Python è una delle principali piattaforme per lo sviluppo di modelli di apprendimento automatico e di intelligenza artificiale.\n",
      "- Sviluppo Web: Python è utilizzato per lo sviluppo di applicazioni web grazie a framework come Django e Flask.\n",
      "- Automazione: Python è utilizzato per automatizzare compiti ripetitivi e complesse operazioni grazie alla sua estensibilità e alla sua facilità di utilizzo.\n",
      "\n",
      "Questi sono i principali utilizzi di Python, ma il linguaggio è anche utilizzato in molti altri ambiti, come la visualizzazione dei dati, la gestione dei database e la sicurezza informatica.\n",
      "\n",
      "--- Test Classe LLMPredictor ---\n",
      "Risposta con classe: **Risposta:**\n",
      "\n",
      "Python è un linguaggio di programmazione molto versatile e polivalente, quindi i suoi utilizzi sono molto ampi e variegati. Tuttavia, in base al contesto fornito, possiamo identificare i principali utilizzi di Python come:\n",
      "\n",
      "1. **Data Science**: Python è ampiamente utilizzato per analisi dei dati, visualizzazione dei dati e modellazione statistica. Le librerie come Pandas, NumPy e Matplotlib lo rendono ideale per queste attività.\n",
      "2. **Machine Learning**: Python è utilizzato per lo sviluppo di modelli di apprendimento automatico, grazie a librerie come scikit-learn, TensorFlow e Keras.\n",
      "3. **Sviluppo Web**: Python è utilizzato per lo sviluppo di applicazioni web grazie a framework come Django e Flask.\n",
      "4. **Automazione**: Python è utilizzato per automatizzare compiti ripetitivi e complessi, grazie alla sua sintassi semplice e alla facilità di integrazione con altri strumenti.\n",
      "\n",
      "In sintesi, Python è un linguaggio di programmazione molto versatile e polivalente, che può essere utilizzato in una vasta gamma di applicazioni, dalle scienze dei dati e machine learning allo sviluppo web e all'automazione.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "from typing import Optional, List, Dict, Any\n",
    "import logging\n",
    "\n",
    "# Configurazione logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LLMPredictor:\n",
    "    \"\"\"\n",
    "    Classe per gestire predizioni con modelli LLM di Hugging Face\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"meta-llama/Meta-Llama-3-8B-Instruct\", \n",
    "                 api_token: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Inizializza il predictor\n",
    "        \n",
    "        Args:\n",
    "            model_name: Nome del modello Hugging Face\n",
    "            api_token: Token API (se None, usa variabile d'ambiente)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.api_token = api_token or os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "        \n",
    "        if not self.api_token:\n",
    "            logger.warning(\"Token API non trovato. Funzionalità potrebbero essere limitate.\")\n",
    "        \n",
    "        # Inizializza il client\n",
    "        self.client = InferenceClient(token=self.api_token)\n",
    "        \n",
    "        # Sistema prompt di default\n",
    "        self.system_prompt = \"Sei un assistente AI esperto che risponde in italiano in modo preciso e utile.\"\n",
    "    \n",
    "    def set_system_prompt(self, prompt: str) -> None:\n",
    "        \"\"\"Imposta un nuovo system prompt\"\"\"\n",
    "        self.system_prompt = prompt\n",
    "    \n",
    "    def create_messages(self, context: str, question: str, \n",
    "                       system_prompt: Optional[str] = None) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Crea la lista di messaggi per la chat completion\n",
    "        \n",
    "        Args:\n",
    "            context: Contesto per rispondere\n",
    "            question: Domanda dell'utente\n",
    "            system_prompt: System prompt personalizzato (opzionale)\n",
    "        \n",
    "        Returns:\n",
    "            Lista di messaggi formattata\n",
    "        \"\"\"\n",
    "        sys_prompt = system_prompt or self.system_prompt\n",
    "        \n",
    "        # Prompt dell'utente più strutturato\n",
    "        user_content = f\"\"\"Ho bisogno di una risposta basata sul seguente contesto.\n",
    "\n",
    "**Contesto:**\n",
    "{context}\n",
    "\n",
    "**Domanda:**\n",
    "{question}\n",
    "\n",
    "Fornisci una risposta accurata e completa basandoti esclusivamente sul contesto fornito.\"\"\"\n",
    "\n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": sys_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ]\n",
    "    \n",
    "    def llm_predict(self, context: str, question: str, \n",
    "                   max_tokens: int = 512, \n",
    "                   temperature: float = 0.7,\n",
    "                   system_prompt: Optional[str] = None) -> str:\n",
    "        \"\"\"\n",
    "        Predice una risposta usando il modello LLM\n",
    "        \n",
    "        Args:\n",
    "            context: Contesto su cui basare la risposta\n",
    "            question: Domanda da porre\n",
    "            max_tokens: Numero massimo di token da generare\n",
    "            temperature: Temperatura per il campionamento (0.0-1.0)\n",
    "            system_prompt: System prompt personalizzato (opzionale)\n",
    "        \n",
    "        Returns:\n",
    "            Risposta generata dal modello\n",
    "        \"\"\"\n",
    "        if not context.strip():\n",
    "            return \"Errore: Il contesto non può essere vuoto.\"\n",
    "        \n",
    "        if not question.strip():\n",
    "            return \"Errore: La domanda non può essere vuota.\"\n",
    "        \n",
    "        try:\n",
    "            # Crea i messaggi\n",
    "            messages = self.create_messages(context, question, system_prompt)\n",
    "            \n",
    "            # Chiamata al modello\n",
    "            completion = self.client.chat_completion(\n",
    "                model=self.model_name,\n",
    "                messages=messages,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=0.9,\n",
    "            )\n",
    "            \n",
    "            # Estrai la risposta\n",
    "            response_text = completion.choices[0].message.content\n",
    "            \n",
    "            if not response_text or response_text.strip() == \"\":\n",
    "                return \"Il modello non ha generato una risposta valida.\"\n",
    "            \n",
    "            return response_text.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Errore nella predizione: {e}\")\n",
    "            return f\"Errore durante la generazione: {e}\"\n",
    "    \n",
    "    def llm_predict_batch(self, contexts_questions: List[tuple], \n",
    "                         max_tokens: int = 512, \n",
    "                         temperature: float = 0.7) -> List[str]:\n",
    "        \"\"\"\n",
    "        Predice risposte per multiple coppie context-question\n",
    "        \n",
    "        Args:\n",
    "            contexts_questions: Lista di tuple (context, question)\n",
    "            max_tokens: Numero massimo di token da generare\n",
    "            temperature: Temperatura per il campionamento\n",
    "        \n",
    "        Returns:\n",
    "            Lista di risposte generate\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for i, (context, question) in enumerate(contexts_questions):\n",
    "            logger.info(f\"Processando richiesta {i+1}/{len(contexts_questions)}\")\n",
    "            result = self.llm_predict(context, question, max_tokens, temperature)\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# Funzione standalone per compatibilità con il tuo codice esistente\n",
    "def llm_predict(context: str, question: str, \n",
    "               max_tokens: int = 512, \n",
    "               temperature: float = 0.7) -> str:\n",
    "    \"\"\"\n",
    "    Funzione standalone per predizioni LLM\n",
    "    Mantiene la compatibilità con il codice esistente\n",
    "    \"\"\"\n",
    "    # Crea un'istanza del predictor\n",
    "    predictor = LLMPredictor()\n",
    "    return predictor.llm_predict(context, question, max_tokens, temperature)\n",
    "\n",
    "\n",
    "# Esempio di utilizzo avanzato\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Test LLM Predictor ===\\n\")\n",
    "    \n",
    "    # Verifica token\n",
    "    token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "    if token:\n",
    "        print(\"✅ Token API trovato\")\n",
    "    else:\n",
    "        print(\"⚠️  Token API non trovato - imposta HUGGINGFACEHUB_API_TOKEN\")\n",
    "        print(\"   Vai su: https://huggingface.co/settings/tokens\")\n",
    "        print(\"   Crea un token 'Read' e impostalo come variabile d'ambiente\\n\")\n",
    "    \n",
    "    # Test base\n",
    "    context_test = \"\"\"\n",
    "    Python è un linguaggio di programmazione ad alto livello, interpretato e orientato agli oggetti.\n",
    "    È noto per la sua sintassi semplice e leggibile. Python è ampiamente utilizzato in \n",
    "    data science, machine learning, sviluppo web e automazione.\n",
    "    \"\"\"\n",
    "    \n",
    "    question_test = \"Quali sono i principali utilizzi di Python?\"\n",
    "    \n",
    "    print(\"--- Test Funzione Standalone ---\")\n",
    "    print(f\"Contesto: {context_test[:100]}...\")\n",
    "    print(f\"Domanda: {question_test}\")\n",
    "    \n",
    "    try:\n",
    "        risposta = llm_predict(context_test, question_test)\n",
    "        print(f\"Risposta: {risposta}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore: {e}\\n\")\n",
    "    \n",
    "    print(\"--- Test Classe LLMPredictor ---\")\n",
    "    \n",
    "    # Test con classe (più flessibile)\n",
    "    predictor = LLMPredictor()\n",
    "    \n",
    "    # Personalizza system prompt\n",
    "    predictor.set_system_prompt(\n",
    "        \"Sei un esperto di tecnologia che fornisce risposte tecniche precise e dettagliate in italiano.\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        risposta_classe = predictor.llm_predict(\n",
    "            context_test, \n",
    "            question_test,\n",
    "            max_tokens=300,\n",
    "            temperature=0.5\n",
    "        )\n",
    "        print(f\"Risposta con classe: {risposta_classe}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore con classe: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a69d91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
